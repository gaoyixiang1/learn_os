# 缺页异常处理源码分析并实践

##  一、缺页异常

mmap()只建立了进程地址空间，在用户空间里可以看到虚拟内存，但没有建立虚拟内存和物理内存之间的关系，==但是当一个进程执行时,如果 CPU 访问到一个有效的虚地址,但是这个地址对应的页没有在内存中,则 CPU产生一个缺页异常。==

同时将这个虚地址存入 CR2 寄存器，然后调用缺页异常处理程序 do_page_fault()。Linux 的缺页异常处理序必须对产生缺页的原因进行区分：是由编程错误所引起的异常，还是由访问进程用户空间的页尚未分配物理页面所引起的异常。

<img src=".\img\page_fault_1.png" alt="image-20231023175011947" style="zoom: 67%;" />

do_page_fault()函数首先读取引起缺页的虚地址。如果没找到,则说明访问了非法虚地址,Linux 会发信号终止进程。否则,检查缺页类型,如果是非法类型(越界错误,段权限错误等)同样会发信号终止进程。

## 二、产生的原因

1 相应的页目录项或页表项为空，也就是线性地址与物理地址的映射关系尚未建立，或者建立后撤销

2.物理页面以及不在内存中

3.指令中规定的访问方式与页面的权限不符

## 三、2.4版本的函数分析

### do_page_fault(/mm/fault.c)

```c
/ * error_code:
 *	bit 0 == 0 means no page found, 1 means protection fault
 *	bit 1 == 0 means read, 1 means write
 *	bit 2 == 0 means kernel, 1 means user-mode
 */
asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long error_code)
    //regs指向缺页发生前的CPU各寄存器的值 error_code值是映射失败的原因
{
	struct task_struct *tsk;    //tsk指向当前进程的task_struct
	struct mm_struct *mm;		//mm指向当前进程的mm
	struct vm_area_struct * vma;//vma指向当前进程的vm_area_struct
	unsigned long address;		
	unsigned long page;
	unsigned long fixup;
	int write;
	siginfo_t info;

	/* get the address */
    //将引发缺页异常的虚地址存入 CR2 寄存器，因此这里通过内嵌获得cr2的地址保存在address
    //只有输出没有输入 将cr2的值保存到address中
	__asm__("movl %%cr2,%0":"=r" (address));

	tsk = current;			//tsk指向当前进程task结构的地址

	/*
	 * We fault-in kernel-space virtual memory on-demand. The
	 * 'reference' page table is init_mm.pgd.
	 *
	 * NOTE! We MUST NOT take any locks for this case. We may
	 * be in an interrupt or a critical region, and should
	 * only copy the information from the master page table,
	 * nothing more.
	 */
    
    //#define TASK_SIZE	(PAGE_OFFSET)默认为3G，address大于3G 即内核空间发生缺页，跳到vmalloc_fault
	if (address >= TASK_SIZE)

		goto vmalloc_fault;

	mm = tsk->mm;//mm指向当前进程的mm
	info.si_code = SEGV_MAPERR;

	/*
	 * If we're in an interrupt or have no user
	 * context, we must not take the fault..
	 */
    //如果映射失败发生在中断或mm=null(映射未建立)，执行no_context;
	if (in_interrupt() || !mm)
		goto no_context;

	down(&mm->mmap_sem);//设置一个信号量mmap_sem，通过down/up进行pv操作
     
//find_vma找到vm_end大于address的第一个区间
	vma = find_vma(mm, address);
     
	if (!vma) //如果找不到，说明这个地址在3G上了,对于用户空间来说，越界了，转向bad_area
		goto bad_area; 
     
	if (vma->vm_start <= address)//如果vma的vm_start <= address，则说明address正好落在vma,映射肯定建立了，转向good_area
		goto good_area;
     //剩下的就是给定地址正落在两个区间当中的空洞里，也就是说映射未建立或被撤销，见下图
     
     
     //VM_GROWSDOWN用于指示堆栈的增长方向 VM_GROWSDOWN=0，说明栈不能向下扩展了，即空洞上方不是堆栈，可能映射被撤销了，因此转向bad_area
	if (!(vma->vm_flags & VM_GROWSDOWN))
		goto bad_area;
    //VM_GROWSDOWN=1,映射成功
	if (error_code & 4) {
        //4=100，bit2=1，表示用户态发生映射失败,检查因堆栈引起的越界
		/*
		 * accessing the stack below %esp is always a bug.
		 * The "+ 32" is there due to some instructions (like
		 * pusha) doing post-decrement on the stack and that
		 * doesn't show up until later..
		 */
        //检查异常地址是否紧挨堆栈指针所指的地方，esp-4(4*8=32b),如果没有紧挨，则转向bad_area
		if (address + 32 < regs->esp)
			goto bad_area;
	}
    //vma指向用户空间堆栈所在区间，expand_stack只是改变了堆栈区的vma，并未建立新的映射
	if (expand_stack(vma, address))
		goto bad_area;
/*
 * Ok, we have a good vm_area for this memory access, so
 * we can handle it..
 */
    / * error_code:
 *	bit 0 == 0 means no page found, 1 means protection fault
 *	bit 1 == 0 means read, 1 means write
 *	bit 2 == 0 means kernel, 1 means user-mode
 */
good_area:
	info.si_code = SEGV_ACCERR;
	write = 0;
    //3=011
	switch (error_code & 3) {
		default:	/* 默认值error_code=11=3,写操作(bit1=1)，protection fault（bit0=1） */
#ifdef TEST_VERIFY_AREA
			if (regs->cs == KERNEL_CS)
				printk("WP fault at %08lx\n", regs->eip);
#endif
			/* fall through */
		case 2:		/* error_code=10=2,写操作(bit1=1)，没物理页（bit0=0） */
			if (!(vma->vm_flags & VM_WRITE))//如果不允许写，则转向bad_area
				goto bad_area;
			write++;
			break;//否则跳出循环
		case 1:		/* error_code=01=1,读操作(bit1=0)， protection fault（bit0=1） */
			goto bad_area;
		case 0:		/* error_code=00=0,读操作(bit1=0)，没物理页（bit0=0）*/
			if (!(vma->vm_flags & (VM_READ | VM_EXEC)))//如果不允许读或调用，则转向bad_area
				goto bad_area;
	}

	/*
	 * If for any reason at all we couldn't handle the fault,
	 * make sure we exit gracefully rather than endlessly redo
	 * the fault.
	 */
     //根据handle_mm_fault的返回值进行处理
	switch (handle_mm_fault(mm, vma, address, write)) {
	case 1://表示页错误由于缺失页面或者页面不可写而发生
		tsk->min_flt++;
		break;
	case 2://表示页错误由于页面存在但是不可访问而发生
		tsk->maj_flt++;
		break;
	case 0://表示页错误无法处理
		goto do_sigbus;
	default:
		goto out_of_memory;//内存不足
	}

	/*
	 * Did it hit the DOS screen memory VA from vm86 mode?
	 */
	if (regs->eflags & VM_MASK) {
		unsigned long bit = (address - 0xA0000) >> PAGE_SHIFT;
		if (bit < 32)
			tsk->thread.screen_bitmap |= 1 << bit;
	}
	up(&mm->mmap_sem);//释放mm->mmap_sem这个信号量
	return;

/*
 * Something tried to access memory that isn't in our memory map..
 * Fix it, but check if it's kernel or user first..
 */
bad_area:
	up(&mm->mmap_sem);//释放mm->mmap_sem这个信号量
   / * error_code:
 *	bit 0 == 0 means no page found, 1 means protection fault
 *	bit 1 == 0 means read, 1 means write
 *	bit 2 == 0 means kernel, 1 means user-mode
 */
bad_area_nosemaphore:
	/* User mode accesses just cause a SIGSEGV */
     //4=100 即（bit0=0 没有物理页，bit1=0 读操作 bit2=1 用户态）
	if (error_code & 4) {
		tsk->thread.cr2 = address;//保存引起页错误的线性地址到cr2中
		tsk->thread.error_code = error_code;//错误码
		tsk->thread.trap_no = 14;
		info.si_signo = SIGSEGV;
		info.si_errno = 0;
		/* info.si_code has been set above */
		info.si_addr = (void *)address;//保存引起页错误的线性地址
		force_sig_info(SIGSEGV, &info, tsk);
		return;
	}

	/*
	 * Pentium F0 0F C7 C8 bug workaround.
	 */
	if (boot_cpu_data.f00f_bug) {
		unsigned long nr;
		
		nr = (address - idt) >> 3;

		if (nr == 6) {
			do_invalid_op(regs, 0);
			return;
		}
	}

no_context:
	/* Are we prepared to handle this kernel fault?  */
	if ((fixup = search_exception_table(regs->eip)) != 0) {
		regs->eip = fixup;
		return;
	}

/*
 * Oops. The kernel tried to access some bad page. We'll have to
 * terminate things with extreme prejudice.
 */

	bust_spinlocks();

	if (address < PAGE_SIZE)
		printk(KERN_ALERT "Unable to handle kernel NULL pointer dereference");
	else
		printk(KERN_ALERT "Unable to handle kernel paging request");
	printk(" at virtual address %08lx\n",address);
	printk(" printing eip:\n");
	printk("%08lx\n", regs->eip);
	asm("movl %%cr3,%0":"=r" (page));
	page = ((unsigned long *) __va(page))[address >> 22];
	printk(KERN_ALERT "*pde = %08lx\n", page);
	if (page & 1) {
		page &= PAGE_MASK;
		address &= 0x003ff000;
		page = ((unsigned long *) __va(page))[address >> PAGE_SHIFT];
		printk(KERN_ALERT "*pte = %08lx\n", page);
	}
	die("Oops", regs, error_code);
	do_exit(SIGKILL);

/*
 * We ran out of memory, or some other thing happened to us that made
 * us unable to handle the page fault gracefully.
 */
out_of_memory:
	up(&mm->mmap_sem);//释放mm->mmap_sem这个信号量
	printk("VM: killing process %s\n", tsk->comm);
	if (error_code & 4)
		do_exit(SIGKILL);
	goto no_context;

do_sigbus:
	up(&mm->mmap_sem);//释放mm->mmap_sem这个信号量

	/*
	 * Send a sigbus, regardless of whether we were in kernel
	 * or user mode.
	 */
	tsk->thread.cr2 = address;
	tsk->thread.error_code = error_code;
	tsk->thread.trap_no = 14;
	info.si_code = SIGBUS;
	info.si_errno = 0;
	info.si_code = BUS_ADRERR;
	info.si_addr = (void *)address;
	force_sig_info(SIGBUS, &info, tsk);

	/* Kernel mode? Handle exceptions or die */
	if (!(error_code & 4))
		goto no_context;
	return;
     
//vmalloc页错误处理函数：
vmalloc_fault:
	{
		/*
		 * Synchronize this task's top level page-table
		 * with the 'reference' page table.
		 */
        //获取页错误地址对于其页目录项的偏移
		int offset = __pgd_offset(address);
		pgd_t *pgd, *pgd_k;
		pmd_t *pmd, *pmd_k;
		//计算address 对应的页目录项 pgd 和内核页目录项 pgd_k
		pgd = tsk->active_mm->pgd + offset;
		pgd_k = init_mm.pgd + offset;
`·//如果不存在，转向bad_area_nosemaphore;
		if (!pgd_present(*pgd)) {
			if (!pgd_present(*pgd_k))
				goto bad_area_nosemaphore;
   //如果存在pgd_k,则设置
			set_pgd(pgd, *pgd_k);
			return;
		}
	//通过pgd计算pmd，通过pgd_k计算pmd_k
		pmd = pmd_offset(pgd, address);
		pmd_k = pmd_offset(pgd_k, address);
	//如果存在pmd 但不存在pmd_k
		if (pmd_present(*pmd) || !pmd_present(*pmd_k))
			goto bad_area_nosemaphore;
      //否则，设置pmd及pmd_k
		set_pmd(pmd, *pmd_k);
		return;
	}
}
```

<img src=".\img\page_fault_2.png" style="zoom: 67%;" />       	

### expand_stack

```c
/* vma is the first one with  address < vma->vm_end,
 * and even  address < vma->vm_start. Have to extend vma. */
//参数 vma 指向一个 vm_area_struct 数据结构，代表着一个区间
static inline int expand_stack(struct vm_area_struct * vma, unsigned long address)
{
	unsigned long grow;
    /* PAGE_SHIFT determines the page size */
//#define PAGE_SHIFT	12
//#define PAGE_SIZE	(1UL << PAGE_SHIFT)
//#define PAGE_MASK	(~(PAGE_SIZE-1))

	address &= PAGE_MASK;
	grow = (vma->vm_start - address) >> PAGE_SHIFT;//确定需要增长几个页面才能把给定的地址包括进去
	//每个进程的 task struct 结构中都有个 rlim 结构数组，规定了对每种资源分配使用的限制，而RLIMIT STACK 就是对用户空间堆栈大小的限制。
    // rlim_cur表示当前进程的资源实际可用的限制
    //if检测扩展以后的区间大小超过了可用于堆栈的资源，或者分配的页面总量超过了可用于该进程的资源限制
    if (vma->vm_end - address > current->rlim[RLIMIT_STACK].rlim_cur ||
	    ((vma->vm_mm->total_vm + grow) << PAGE_SHIFT) > current->rlim[RLIMIT_AS].rlim_cur)
		return -ENOMEM;
    //若没有出错，则改变堆栈vm_area_struct的结构
	vma->vm_start = address;
	vma->vm_pgoff -= grow;
	vma->vm_mm->total_vm += grow;
	if (vma->vm_flags & VM_LOCKED)
		vma->vm_mm->locked_vm += grow;
	return 0;
}
```

### handle_mm_fault

```c
int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct * vma,
	unsigned long address, int write_access)
    //给定mm指向当前进程的mm_struct vma指向当前进程的vm_area_struct address给定地址 write_access表示是否可写
{
	int ret = -1;
	pgd_t *pgd;
	pmd_t *pmd;
//由pgd_offset计算出给定地址所属页面目录项的指针
    //#define pgd_offset(mm，address) ((mm)->pgd+pgd_index(address))
   // #define pgd_index(address) ((address >> PGDIR_SHIFT) & (PTRS_PER_PGD-1))
	pgd = pgd_offset(mm, address);
    //pmd_alloc分配页中间项
	pmd = pmd_alloc(pgd, address);
//如果分配成功
	if (pmd) {
		pte_t * pte = pte_alloc(pmd, address);
		if (pte)
			ret = handle_pte_fault(mm, vma, address, write_access, pte);
	}
	return ret;
}

```

### 函数调用流程图：

<img src=".\img\page_fault_3.png" style="zoom: 67%;" />     

## 四、5.0版本的函数分析

### do_page_fault    (arch/x86/mm/fault.c)

```c
dotraplinkage void notrace
do_page_fault(struct pt_regs *regs, unsigned long error_code)
{
	unsigned long address = read_cr2(); /* 从cr2获得缺页异常的地址 */
	enum ctx_state prev_state;

	prev_state = exception_enter();//获取当前进程的上下文
	if (trace_pagefault_enabled())//如果启用缺页跟踪，则直接调用trace_page_fault_entries来获取相关信息
		trace_page_fault_entries(address, regs, error_code);
	//否则就调用__do_page_fault函数
	__do_page_fault(regs, error_code, address);
	exception_exit(prev_state);//退出当前状态，返回到prev_state
}
```

### __do_page_fault

```c
static noinline void
__do_page_fault(struct pt_regs *regs, unsigned long hw_error_code,
		unsigned long address)
{
	prefetchw(&current->mm->mmap_sem);//获取当前进程的mmap_sem信号量

	if (unlikely(kmmio_fault(regs, address)))
		return;

	/* Was the fault on kernel-controlled part of the address space? */
	if (unlikely(fault_in_kernel_space(address)))//检查页错误是否发生在内核空间
		do_kern_addr_fault(regs, hw_error_code, address);//处理内核空间的页错误
	else
		do_user_addr_fault(regs, hw_error_code, address);//处理用户空间的页错误
}
```

### do_kern_addr_fault

```c
do_kern_addr_fault(struct pt_regs *regs, unsigned long hw_error_code,
		   unsigned long address)
{
	/*
	 * Protection keys exceptions only happen on user pages.  We
	 * have no user pages in the kernel portion of the address
	 * space, so do not expect them here.
	 */
    //通过WARN_ON_ONCE来检查是否发生了Protection keys异常
	WARN_ON_ONCE(hw_error_code & X86_PF_PK);

	/*
	 * We can fault-in kernel-space virtual memory on-demand. The
	 * 'reference' page table is init_mm.pgd.
	 *
	 * NOTE! We MUST NOT take any locks for this case. We may
	 * be in an interrupt or a critical region, and should
	 * only copy the information from the master page table,
	 * nothing more.
	 *
	 * Before doing this on-demand faulting, ensure that the
	 * fault is not any of the following:
	 * 1. A fault on a PTE with a reserved bit set.
	 * 2. A fault caused by a user-mode access.  (Do not demand-
	 *    fault kernel memory due to user-mode accesses).
	 * 3. A fault caused by a page-level protection violation.
	 *    (A demand fault would be on a non-present page which
	 *     would have X86_PF_PROT==0).
	 */
    //如果错误码中的X86_PF_RSVD X86_PF_USER X86_PF_PROT都为0，说明页错误不是由保留位设置、用户模式访问或页级保护违规引起的
	if (!(hw_error_code & (X86_PF_RSVD | X86_PF_USER | X86_PF_PROT))) {
        //如果成功调用vmalloc_fault函数，则返回
		if (vmalloc_fault(address) >= 0)
			return;
	}

	/* Was the fault spurious, caused by lazy TLB invalidation? */
    //检查是否由于TLB表无效引起的异常
	if (spurious_kernel_fault(hw_error_code, address))
		return;

	/* kprobes don't want to hook the spurious faults: */
    //检查是否由于kprobs引起的
	if (kprobes_fault(regs))
		return;

	/*
	 * Note, despite being a "bad area", there are quite a few
	 * acceptable reasons to get here, such as erratum fixups
	 * and handling kernel code that can fault, like get_user().
	 *
	 * Don't take the mm semaphore here. If we fixup a prefetch
	 * fault we could otherwise deadlock:
	 */
    //调用bad_area_nosemaphore处理
	bad_area_nosemaphore(regs, hw_error_code, address);
}
```

### do_user_addr_fault

```c
void do_user_addr_fault(struct pt_regs *regs,
			unsigned long hw_error_code,
			unsigned long address)
{
	struct vm_area_struct *vma;
	struct task_struct *tsk;
	struct mm_struct *mm;
	vm_fault_t fault, major = 0;
	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;//处理页面故障的标志位

	tsk = current;
	mm = tsk->mm;

	/* kprobes don't want to hook the spurious faults: */
	if (unlikely(kprobes_fault(regs)))//如果是kprobs引发的错误
		return;

	/*
	 * Reserved bits are never expected to be set on
	 * entries in the user portion of the page tables.
	 */
    //如果是保留位引起的错误，则pgtable_bad，因为保留位是不该被出现在页表中
	if (unlikely(hw_error_code & X86_PF_RSVD))
		pgtable_bad(regs, hw_error_code, address);

	/*
	 * If SMAP is on, check for invalid kernel (supervisor) access to user
	 * pages in the user address space.  The odd case here is WRUSS,
	 * which, according to the preliminary documentation, does not respect
	 * SMAP and will have the USER bit set so, in all cases, SMAP
	 * enforcement appears to be consistent with the USER bit.
	 */
    //smap开启且错误不是在用户模式下出错，以及regs的flags没有设置X86_EFLAGS_AC
	if (unlikely(cpu_feature_enabled(X86_FEATURE_SMAP) &&
		     !(hw_error_code & X86_PF_USER) &&
		     !(regs->flags & X86_EFLAGS_AC)))
	{
		bad_area_nosemaphore(regs, hw_error_code, address);
		return;
	}

	/*
	 * If we're in an interrupt, have no user context or are running
	 * in a region with pagefaults disabled then we must not take the fault
	 */
    //如果处于中断或者mm空间不存在，即没有建立映射，则执行bad_area_nosemaphore
	if (unlikely(faulthandler_disabled() || !mm)) {
		bad_area_nosemaphore(regs, hw_error_code, address);
		return;
	}

	/*
	 * It's safe to allow irq's after cr2 has been saved and the
	 * vmalloc fault has been handled.
	 *
	 * User-mode registers count as a user access even for any
	 * potential system fault or CPU buglet:
	 */
	if (user_mode(regs)) {//处于用户模式
		local_irq_enable();//如果处于用户态调用local_irq_enable允许中断，并设置了FAULT_FLAG_USER，表示用户模式的中断
		flags |= FAULT_FLAG_USER;
	} else {
        //处于内核模式
        //检查regs->flags的X86_EFLAGS_IF位如果为1，则调用local_irq_enable允许中断
		if (regs->flags & X86_EFLAGS_IF)
			local_irq_enable();
	}

	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);

	if (hw_error_code & X86_PF_WRITE)
		flags |= FAULT_FLAG_WRITE; //该错误是写入内存引起
	if (hw_error_code & X86_PF_INSTR)
		flags |= FAULT_FLAG_INSTRUCTION;//该错误是指令引起

#ifdef CONFIG_X86_64  //如果定义CONFIG_X86_64
	/*
	 * Instruction fetch faults in the vsyscall page might need
	 * emulation.  The vsyscall page is at a high address
	 * (>PAGE_OFFSET), but is considered to be part of the user
	 * address space.
	 *
	 * The vsyscall page does not have a "real" VMA, so do this
	 * emulation before we go searching for VMAs.
	 */
    //如果错误由指令引起，并且address处于vsyscall页中，则调用emulate_vsyscall
	if ((hw_error_code & X86_PF_INSTR) && is_vsyscall_vaddr(address)) {
		if (emulate_vsyscall(regs, address))
			return;
	}
#endif

	/*
	 * Kernel-mode access to the user address space should only occur
	 * on well-defined single instructions listed in the exception
	 * tables.  But, an erroneous kernel fault occurring outside one of
	 * those areas which also holds mmap_sem might deadlock attempting
	 * to validate the fault against the address space.
	 *
	 * Only do the expensive exception table search when we might be at
	 * risk of a deadlock.  This happens if we
	 * 1. Failed to acquire mmap_sem, and
	 * 2. The access did not originate in userspace.
	 */
    //如果没有立即成功读取mmap_sem信号量
	if (unlikely(!down_read_trylock(&mm->mmap_sem))) {
       // 并且不是在用户模式下并且在异常表中不存在导致页面错误的指令,则bad_area_nosemaphore
        //regs->ip 保存导致页面错误的指令的指令指针
		if (!user_mode(regs) && !search_exception_tables(regs->ip)) {
			/*
			 * Fault from code in kernel from
			 * which we do not expect faults.
			 */
			bad_area_nosemaphore(regs, hw_error_code, address);
			return;
		}
retry:
		down_read(&mm->mmap_sem); //读取mmap_sem信号量
	} else {
		/*
		 * The above down_read_trylock() might have succeeded in
		 * which case we'll have missed the might_sleep() from
		 * down_read():
		 */
		might_sleep();//睡眠
	}

	vma = find_vma(mm, address);//find_vma找到vm_end大于address的第一个区间
	if (unlikely(!vma)) {//如果没找到，则执行bad_area
		bad_area(regs, hw_error_code, address);
		return;
	}
    //如果address介于vm_start和vm_end之间，则转向good_area
	if (likely(vma->vm_start <= address))
		goto good_area;
    //如果VM_GROWSDOWN=0，空洞上方不是堆栈，可能映射被撤销了，因此转向bad_area
	if (unlikely(!(vma->vm_flags & VM_GROWSDOWN))) {
		bad_area(regs, hw_error_code, address);
		return;
	}
    //如果扩展堆栈错误
	if (unlikely(expand_stack(vma, address))) {
		bad_area(regs, hw_error_code, address);
		return;
	}

	/*
	 * Ok, we have a good vm_area for this memory access, so
	 * we can handle it..
	 */
good_area:
    //如果发生访问错误，调用bad_area_access_error
	if (unlikely(access_error(hw_error_code, vma))) {
		bad_area_access_error(regs, hw_error_code, address, vma);
		return;
	}

	/*
	 * If for any reason at all we couldn't handle the fault,
	 * make sure we exit gracefully rather than endlessly redo
	 * the fault.  Since we never set FAULT_FLAG_RETRY_NOWAIT, if
	 * we get VM_FAULT_RETRY back, the mmap_sem has been unlocked.
	 *
	 * Note that handle_userfault() may also release and reacquire mmap_sem
	 * (and not return with VM_FAULT_RETRY), when returning to userland to
	 * repeat the page fault later with a VM_FAULT_NOPAGE retval
	 * (potentially after handling any pending signal during the return to
	 * userland). The return to userland is identified whenever
	 * FAULT_FLAG_USER|FAULT_FLAG_KILLABLE are both set in flags.
	 */
    //handle_mm_fault是用来分配页框的，他返回一个vm_fault_t类型的值
	fault = handle_mm_fault(vma, address, flags);
	major |= fault & VM_FAULT_MAJOR;//如果返回值中设置了 VM_FAULT_MAJOR，则让major+1

	/*
	 * If we need to retry the mmap_sem has already been released,
	 * and if there is a fatal signal pending there is no guarantee
	 * that we made any progress. Handle this case first.
	 */
    //如果返回值中设置了 VM_FAULT_RETRY
	if (unlikely(fault & VM_FAULT_RETRY)) {
		/* Retry at most once */
        //标志位允许retry
		if (flags & FAULT_FLAG_ALLOW_RETRY) {
            //重新复位相应的标志位
			flags &= ~FAULT_FLAG_ALLOW_RETRY;
			flags |= FAULT_FLAG_TRIED;
			if (!fatal_signal_pending(tsk))//如果当前任务没有未处理的致命信号
				goto retry; //跳到retry
		}

		/* User mode? Just return to handle the fatal exception */
		if (flags & FAULT_FLAG_USER)//错误在用户模式发生
			return;

		/* Not returning to user mode? Handle exceptions or die: */
        //否则在内核模式发生，调用no_context
		no_context(regs, hw_error_code, address, SIGBUS, BUS_ADRERR);
		return;
	}

	up_read(&mm->mmap_sem);//释放信号量
	if (unlikely(fault & VM_FAULT_ERROR)) {
		mm_fault_error(regs, hw_error_code, address, fault);
		return;
	}

	/*
	 * Major/minor page fault accounting. If any of the events
	 * returned VM_FAULT_MAJOR, we account it as a major fault.
	 */
    //发生了major page fault
	if (major) {
		tsk->maj_flt++;//maj_flt计数器++
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);
	} else {
        //minor page fault
		tsk->min_flt++;//min_flt计数器++
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);
	}

	check_v8086_mode(regs, address, tsk);
}
```

### handle_mm_fault

```c
vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
		unsigned int flags)
{
	vm_fault_t ret;

	__set_current_state(TASK_RUNNING);//设置进程执行状态为运行

	count_vm_event(PGFAULT);//对页面错误进行计数+1
	count_memcg_event_mm(vma->vm_mm, PGFAULT);

	/* do counter updates before entering really critical section. */
	check_sync_rss_stat(current);//更新计数器

	if (!arch_vma_access_permitted(vma, flags & FAULT_FLAG_WRITE,
					    flags & FAULT_FLAG_INSTRUCTION,
					    flags & FAULT_FLAG_REMOTE))
        //检查当前进程是否有权限访问,写入该虚拟内存区域（VMA）如果没有权限，返回VM_FAULT_SIGSEGV错误
		return VM_FAULT_SIGSEGV;

	/*
	 * Enable the memcg OOM handling for faults triggered in user
	 * space.  Kernel faults are handled more gracefully.
	 */
	if (flags & FAULT_FLAG_USER)//用户空间中出现的异常，调用Out of Memory
		mem_cgroup_enter_user_fault();

	if (unlikely(is_vm_hugetlb_page(vma)))//处理比较大的页、调用ugetlb_fault处理
		ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
	else//处理一般的页的函数，调用__handle_mm_fault
		ret = __handle_mm_fault(vma, address, flags);
//用户空间中出现的异常，则退出Out of Memory
	if (flags & FAULT_FLAG_USER) {
		mem_cgroup_exit_user_fault();
		/*
		 * The task may have entered a memcg OOM situation but
		 * if the allocation error was handled gracefully (no
		 * VM_FAULT_OOM), there is no need to kill anything.
		 * Just clean up the OOM state peacefully.
		 */
        //如果当前进程正在执行oom并且没有出现VM_FAULT_OOM的错误
		if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
			mem_cgroup_oom_synchronize(false);
	}

	return ret;
}
```

### __handle_mm_fault

```c
static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
		unsigned long address, unsigned int flags)
{
	struct vm_fault vmf = {
		.vma = vma,
		.address = address & PAGE_MASK,
		.flags = flags,
		.pgoff = linear_page_index(vma, address),
		.gfp_mask = __get_fault_gfp_mask(vma),
	};
	unsigned int dirty = flags & FAULT_FLAG_WRITE;
	struct mm_struct *mm = vma->vm_mm;
	pgd_t *pgd;
	p4d_t *p4d;
	vm_fault_t ret;

	pgd = pgd_offset(mm, address);
	p4d = p4d_alloc(mm, pgd, address);
	if (!p4d)
		return VM_FAULT_OOM;

	vmf.pud = pud_alloc(mm, p4d, address);
	if (!vmf.pud)
		return VM_FAULT_OOM;
	if (pud_none(*vmf.pud) && __transparent_hugepage_enabled(vma)) {
		ret = create_huge_pud(&vmf);
		if (!(ret & VM_FAULT_FALLBACK))
			return ret;
	} else {
		pud_t orig_pud = *vmf.pud;

		barrier();
		if (pud_trans_huge(orig_pud) || pud_devmap(orig_pud)) {

			/* NUMA case for anonymous PUDs would go here */

			if (dirty && !pud_write(orig_pud)) {
				ret = wp_huge_pud(&vmf, orig_pud);
				if (!(ret & VM_FAULT_FALLBACK))
					return ret;
			} else {
				huge_pud_set_accessed(&vmf, orig_pud);
				return 0;
			}
		}
	}

	vmf.pmd = pmd_alloc(mm, vmf.pud, address);
	if (!vmf.pmd)
		return VM_FAULT_OOM;
	if (pmd_none(*vmf.pmd) && __transparent_hugepage_enabled(vma)) {
		ret = create_huge_pmd(&vmf);
		if (!(ret & VM_FAULT_FALLBACK))
			return ret;
	} else {
		pmd_t orig_pmd = *vmf.pmd;

		barrier();
		if (unlikely(is_swap_pmd(orig_pmd))) {
			VM_BUG_ON(thp_migration_supported() &&
					  !is_pmd_migration_entry(orig_pmd));
			if (is_pmd_migration_entry(orig_pmd))
				pmd_migration_entry_wait(mm, vmf.pmd);
			return 0;
		}
		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
			if (pmd_protnone(orig_pmd) && vma_is_accessible(vma))
				return do_huge_pmd_numa_page(&vmf, orig_pmd);

			if (dirty && !pmd_write(orig_pmd)) {
				ret = wp_huge_pmd(&vmf, orig_pmd);
				if (!(ret & VM_FAULT_FALLBACK))
					return ret;
			} else {
				huge_pmd_set_accessed(&vmf, orig_pmd);
				return 0;
			}
		}
	}

	return handle_pte_fault(&vmf);
}
```

### 本机的函数调用流程图如图所示：

<img src=".\img\page_fault_4.png" style="zoom: 67%;" />     

利用perf工具查看：

<img src=".\img\page_fault_5.png" alt="image-20231024204159023" style="zoom:50%;" />

## 五、perf

次分析函数执行流程的工具为perf，是在张铭轩同学的帮助下完成的，这块我简单分析一下perf的安装及使用，也为之后的学习做个笔记

如何在虚拟机中安装perf：

首先，通过`uname -r`查看本机的内核版本

在[清华源](https://mirrors.tuna.tsinghua.edu.cn/kernel)下载对应内核版本的源码，然后在虚拟机下解压，并进入源码下的/tools/perf 执行sudo make

执行下列命令：

```
sudo apt-get update
sudo apt install flex
sudo apt install bision
sudo make
```

`perf top` 实时显示系统/进程的性能统计信息

`perf stat`用于统计event出现的次数

`perf record`是采样模式，`perf`收集采样信息并记录在文件中，可以离线分析。

`perf report` 主要用来分析上面`perf record`生成的`perf.data`文件。

## 六、统计缺页异常执行时间

代码：

### fault.h

```c
/* SPDX-License-Identifier: (LGPL-2.1 OR BSD-2-Clause) */
/* Copyright (c) 2020 Facebook */
#ifndef __BOOTSTRAP_H
#define __BOOTSTRAP_H

#define TASK_COMM_LEN	 16
#define MAX_FILENAME_LEN 127

struct event {
	unsigned int pid;
	unsigned long long duration_ns;
	char comm[TASK_COMM_LEN];
    unsigned exit_code;
	bool exit_event;
};

#endif /* __BOOTSTRAP_H */
```

### fault.bpf.c

```c
// SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause
/* Copyright (c) 2020 Facebook */
#include "vmlinux.h"
#include <bpf/bpf_helpers.h>
#include <bpf/bpf_tracing.h>
#include <bpf/bpf_core_read.h>
#include "fault.h"

char LICENSE[] SEC("license") = "Dual BSD/GPL";

struct {
	__uint(type, BPF_MAP_TYPE_HASH);
	__uint(max_entries, 8192);
	__type(key, pid_t);
	__type(value, u64);
} exec_start SEC(".maps");

struct {
	__uint(type, BPF_MAP_TYPE_RINGBUF);
	__uint(max_entries, 256 * 1024);
} rb SEC(".maps");

// const volatile unsigned long long min_duration_ns = 0;

SEC("kprobe/handle_mm_fault")
int BPF_KPROBE(handle_mm_fault_enter)
{
	unsigned int pid;
	u64 ts;

	/* remember time exec() was executed for this PID */
	pid = bpf_get_current_pid_tgid() >> 32;
	ts = bpf_ktime_get_ns();
	bpf_map_update_elem(&exec_start, &pid, &ts, BPF_ANY);

	
	return 0;
}

SEC("kretprobe/handle_mm_fault")
int BPF_KRETPROBE(handle_mm_fault_exit)
{
	struct task_struct *task;
    task = (struct task_struct *)bpf_get_current_task();
	struct event *e;
	unsigned int pid;
	u64  *start_ts, duration_ns = 0;
    char comm[20];

	/* get PID and TID of exiting thread/process */
	pid = bpf_get_current_pid_tgid();
	
	/* if we recorded start of the process, calculate lifetime duration */
	start_ts = bpf_map_lookup_elem(&exec_start, &pid);
	if (!start_ts) return 0;

		duration_ns = bpf_ktime_get_ns() - *start_ts;

	bpf_map_delete_elem(&exec_start, &pid);

	/* reserve sample from BPF ringbuf */
	e = bpf_ringbuf_reserve(&rb, sizeof(*e), 0);
	if (!e)
		return 0;

	/* fill out the sample with data */
	

	// e->exit_event = true;
	e->duration_ns = duration_ns;
	e->pid = pid;
	bpf_get_current_comm(&e->comm, sizeof(e->comm));
    e->exit_code = (BPF_CORE_READ(task, exit_code) >> 8) & 0xff;
	/* send data to user-space for post-processing */
	bpf_ringbuf_submit(e, 0);
	return 0;
}

```

### fault.c

```c
// SPDX-License-Identifier: (LGPL-2.1 OR BSD-2-Clause)
/* Copyright (c) 2020 Facebook */
#include <argp.h>
#include <signal.h>
#include <stdio.h>
#include <time.h>
#include <sys/resource.h>
#include <bpf/libbpf.h>
#include "fault.h"
#include "fault.skel.h"


static int libbpf_print_fn(enum libbpf_print_level level, const char *format, va_list args)
{

	return vfprintf(stderr, format, args);
}

static volatile bool exiting = false;

static void sig_handler(int sig)
{
	exiting = true;
}

static int handle_event(void *ctx, void *data, size_t data_sz)
{
	const struct event *e = data;
	printf("%-8d %-10d %-8s %-8lld\n", e->pid, e->exit_code, e->comm,e->duration_ns);

	return 0;
}

int main(int argc, char **argv)
{
	struct ring_buffer *rb = NULL;
	struct fault_bpf *skel;
	int err;

	/* Parse command line arguments */
	

	/* Set up libbpf errors and debug info callback */
	libbpf_set_print(libbpf_print_fn);

	/* Cleaner handling of Ctrl-C */
	signal(SIGINT, sig_handler);
	signal(SIGTERM, sig_handler);

	/* Load and verify BPF application */
	skel = fault_bpf__open();
	if (!skel) {
		fprintf(stderr, "Failed to open and load BPF skeleton\n");
		return 1;
	}

	/* Parameterize BPF code with minimum duration parameter */
	// skel->rodata->min_duration_ns = env.min_duration_ms * 1000000ULL;

	/* Load & verify BPF programs */
	err = fault_bpf__load(skel);
	if (err) {
		fprintf(stderr, "Failed to load and verify BPF skeleton\n");
		goto cleanup;
	}

	/* Attach tracepoints */
	err = fault_bpf__attach(skel);
	if (err) {
		fprintf(stderr, "Failed to attach BPF skeleton\n");
		goto cleanup;
	}

	/* Set up ring buffer polling */
	rb = ring_buffer__new(bpf_map__fd(skel->maps.rb), handle_event, NULL, NULL);
	if (!rb) {
		err = -1;
		fprintf(stderr, "Failed to create ring buffer\n");
		goto cleanup;
	}
	/* Process events */
	printf("%-8s %-6s %-8s %-8s\n", "PID", "EXIT_CODE", "COMM", "DURATION_NS");
	while (!exiting) {
		err = ring_buffer__poll(rb, 100 /* timeout, ms */);
		/* Ctrl-C will cause -EINTR */
		if (err == -EINTR) {
			err = 0;
			break;
		}
		if (err < 0) {
			printf("Error polling perf buffer: %d\n", err);
			break;
		}
	}

cleanup:
	/* Clean up */
	ring_buffer__free(rb);
	fault_bpf__destroy(skel);

	return err < 0 ? -err : 0;
}

```

### 运行结果

<img src=".\img\page_fault_6.png" alt="image-20231025124650544" style="zoom:50%;" />
