# 伙伴算法

## 一、前言

在Linux当中，CPU访问的地址不是物理地址而是虚拟内存空间的虚地址。因此对于内存页面的管理，通常是先在虚拟内存空间中分配一个虚存区，然后再根据需要为此区间分配相应的物理页面并建立映射。

页描述符为`struct page`结构，在`/include/linux/mm_types.h`中定义，查看其具体的定义，如下：

```c
struct page {
	
		struct {	/* page_pool used by netstack */
			/**
			 * @pp_magic: magic value to avoid recycling non
			 * page_pool allocated pages.
			 */
			unsigned long pp_magic;
			struct page_pool *pp;
			unsigned long _pp_mapping_pad;
			unsigned long dma_addr;
			union {
				/**
				 * dma_addr_upper: might require a 64-bit
				 * value on 32-bit architectures.
				 */
				unsigned long dma_addr_upper;
				/**
				 * For frag page support, not supported in
				 * 32-bit architectures with 64-bit DMA.
				 */
				atomic_long_t pp_frag_count;
			};
		};
		struct {	/* Tail pages of compound page */
			unsigned long compound_head;	/* Bit zero is set */

			/* First tail page only */
			unsigned char compound_dtor;
			unsigned char compound_order;
			atomic_t compound_mapcount;
			atomic_t subpages_mapcount;
			atomic_t compound_pincount;

		};
		struct {	/* Second tail page of transparent huge page */
			unsigned long _compound_pad_1;	/* compound_head */
			unsigned long _compound_pad_2;
			/* For both global and memcg */
			struct list_head deferred_list;
		};
		struct {	/* Second tail page of hugetlb page */
			unsigned long _hugetlb_pad_1;	/* compound_head */
			void *hugetlb_subpool;
			void *hugetlb_cgroup;
			void *hugetlb_cgroup_rsvd;
			void *hugetlb_hwpoison;
			/* No more space on 32-bit: use third tail if more */
		};
		struct {	/* Page table pages */
			unsigned long _pt_pad_1;	/* compound_head */
			pgtable_t pmd_huge_pte; /* protected by page->ptl */
			unsigned long _pt_pad_2;	/* mapping */
			union {
				struct mm_struct *pt_mm; /* x86 pgds only */
				atomic_t pt_frag_refcount; /* powerpc */
			};
		};
		struct {	/* ZONE_DEVICE pages */
			/** @pgmap: Points to the hosting device page map. */
			struct dev_pagemap *pgmap;
			void *zone_device_data;
			/*
			 * ZONE_DEVICE private pages are counted as being
			 * mapped so the next 3 words hold the mapping, index,
			 * and private fields from the source anonymous or
			 * page cache page while the page is migrated to device
			 * private memory.
			 * ZONE_DEVICE MEMORY_DEVICE_FS_DAX pages also
			 * use the mapping, index, and private fields when
			 * pmem backed DAX files are mapped.
			 */
		};

		/** @rcu_head: You can use this to free a page by RCU. */
		struct rcu_head rcu_head;
	};

	union {		/* This union is 4 bytes in size. */
		/*
		 * If the page can be mapped to userspace, encodes the number
		 * of times this page is referenced by a page table.
		 */
		atomic_t _mapcount;

		/*
		 * If the page is neither PageSlab nor mappable to userspace,
		 * the value stored here may help determine what this page
		 * is used for.  See page-flags.h for a list of page types
		 * which are currently stored here.
		 */
		unsigned int page_type;
	};

	/* Usage count. *DO NOT USE DIRECTLY*. See page_ref.h */
	atomic_t _refcount;

} _struct_page_alignment;
```

## 二、伙伴算法的引入

随着用户程序的执行和结束,需要不断地为其分配和释放物理页面。内核应该为了分配一组连续的页面而建立一种稳定、高效的分配策略。但是,频繁地请求和释放不同大小的一组连续页面，必然导致在已分配的内存块中分散许多小块的空闲页面,即外碎片,由此带来的问题是，即使这些小块的空闲页面加起来足以满足所请求的页面，但是要分配一个大块的连续页面可能就根本无法满足。为此,Linux 采用著名的伙伴(Buddy)算法来解决外碎片问。

<img src=".\img\Buddy_1.png" alt="image-20231026222931182" style="zoom:67%;" />

伙伴系统所采用的数据结构是一个叫做 free_area的数组，示意图如上所示，定义如下：

```c
struct zone {
	...
	/* free areas of different sizes */
	struct free_area	free_area[MAX_ORDER];//MAX_ORDER=11

	...
} ____cacheline_internodealigned_in_smp;
```



其中链表中的每一项定义如下：

```c
struct free_area {
	struct list_head	free_list[MIGRATE_TYPES];//当前分配的对应的页
	unsigned long		nr_free;//当前链表中空闲页框块的数目
};
```

## 三、伙伴算法的工作原理

伙伴算法的分配原理是，如果分配阶为n的页框块，那么先从第n条页框块链表中查找是否存在这么大小的空闲页块。如果有则分配，否则在第n+1条链表中继续查找，直到找到为止。

<img src=".\img\Buddy_2.png" alt="image-20231029170229493" style="zoom:50%;" />

假设要求分配的块其大小为 128 个页面。该算法先在块大小为 128 个页面的链表中查找，看是否有这样一个空闲块。如果有，就直接分配。如果没有，该算法会查找下一个更大的块，也就是大小256的页面的块，如果存在这样的空闲块，内核就把这 256 个页面分为两等份，一份分配出去另一份插人到块大小为 128 个页面的链表中。如果在块大小为 256 个页面的链表中也没有找到空闲页块，就继续找更大的块，即512个页面的块。如果存在这样的块，内核就从512 个页面的块中分出128 个页面满足请求，然后从 384 个页面中取出 256 个页面插人到块大小为 256 个页面的链表中。然后把剩余的 128 个页面插入到块大小为 128 个页面的链表中。如果 512个页面的链表中还没有空闲块，该算法就放弃分配，并发出出错信号。

## 四、6.2版本的伙伴算法

从硬件的角度说存在两种不同的机器，分别用两种不同的方式来管理机器。一种是UMA，另一种是NUMA，其示意图为：UMA指的是多个cpu共享一个内存，NUMA指的是每个cpu有自己的本地内存，然后处理器通过总线连接起来，进而可以访问其他cpu的本地内存。

<img src=".\img\Buddy_3.png" alt="image-20231029170043271" style="zoom: 67%;" />

Linux引入了一个概念称为node，一个node对应一个内存块，对于UMA系统，只有一个Node.其对应的数据结构为struct pglist_data。对于NUMA系统来讲，对于NUMA结构来说，因为有多个节点，因此各节点之间形成一个链表。每个节点又被划分几个内存管理区（ZONES），在一个内存管理区中则是一个个的页框。

各个节点划分为若干个区，也是对物理内存的进一步细分。通过下面几个宏来标记物理内存不同的区：ZONE_DMA：标记适合DMA的内存区。ZONE_NORMAL：可以直接映射到内核空间的物理内存。ZONE_HIGHMEM：高端物理内存。

<img src=".\img\Buddy_4.png" alt="image-20231030124222950" style="zoom:50%;" />

关于伙伴算法分配的相关信息，可以通过`cat /proc/buddyinfo`来查看

![image-20231030123641226](.\img\Buddy_5.png)

函数__get_free_pages 用于物理页块的分配，其定义如下:

### 4.1 __get_free_pages

其中gfp_mask是分配标志，表示对所分配内存的特殊要求，常用的标志有GFP_KERNEL（在分配内存期间可以睡眠）和GFP_ATOMIC（在非配内存期间不能睡眠），order是指数，表示free_area[order]页面大小2的order次方。

```c
/mm/page_alloc.c
    
unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
{
	struct page *page;
    
	//__GFP_HIGHMEM是表示高端内存分配的标志位，因此这行代码的意思是从gfp_mask里面把__GFP_HIGHMEM=0
	page = alloc_pages(gfp_mask & ~__GFP_HIGHMEM, order);//调用alloc_pages分配2的order次大小的页块
	if (!page)
		return 0;
	return (unsigned long) page_address(page);//返回分配页块的起始地址
}
```

### 4.2 alloc_pages

alloc_pages有两个函数，分别在路径/mm/mempolicy.c和路径include/linux/gfp.h下：

#### 4.2.1 /include/linux/gfp.h

普通环境下alloc_pages执行流程如下：

<img src=".\img\Buddy_6.png" alt="image-20231028115339321" style="zoom: 50%;" />



```c
/include/linux/gfp.h

static inline struct page *alloc_pages(gfp_t gfp_mask, unsigned int order)
{
    //调用alloc_pages_node返回分配的页的结点
	return alloc_pages_node(numa_node_id(), gfp_mask, order);
}
```

alloc_pages_node定义如下：

```c
static inline struct page *alloc_pages_node(int nid, gfp_t gfp_mask,
						unsigned int order)
{
	if (nid == NUMA_NO_NODE)//#define	NUMA_NO_NODE	(-1)，即不存在
		nid = numa_mem_id();//获取节点id

	return __alloc_pages_node(nid, gfp_mask, order);//调用__alloc_pages_node
}
```

__alloc_pages_node定义如下：

```c
static inline struct page *
__alloc_pages_node(int nid, gfp_t gfp_mask, unsigned int order)
{
	VM_BUG_ON(nid < 0 || nid >= MAX_NUMNODES);//nid是否合法
	warn_if_node_offline(nid, gfp_mask);//如果node处于offline，则警告

	return __alloc_pages(gfp_mask, order, nid, NULL);//调用__alloc_pages
}
```



#### 4.2.2 /mm/mempolicy.c

NUME的alloc_pages执行流程如下

<img src=".\img\Buddy_7.png" alt="image-20231028115844330" style="zoom:50%;" />

```c
/mm/mempolicy.c   **NUME**
    
struct page *alloc_pages(gfp_t gfp, unsigned order)
{
	struct mempolicy *pol = &default_policy;//默认为NUME的调度策略
	struct page *page;
	//如果不是在中断处理程序中调用并且gfp_mask中__GFP_THISNODE=0，__GFP_THISNODE通常在NUME使用
	if (!in_interrupt() && !(gfp & __GFP_THISNODE))
		pol = get_task_policy(current);//获取当前进程的分配策略

	/*
	 * No reference counting needed for current->mempolicy
	 * nor system default_policy
	 */
    //如果内存分配策略模式为MPOL_INTERLEAVE，则调用interleave_nodes返回内存分配的节点
	if (pol->mode == MPOL_INTERLEAVE)
		page = alloc_page_interleave(gfp, order, interleave_nodes(pol));
	else if (pol->mode == MPOL_PREFERRED_MANY)
		page = alloc_pages_preferred_many(gfp, order,
				  policy_node(gfp, pol, numa_node_id()), pol);
	else
        //调用 __alloc_pages
		page = __alloc_pages(gfp, order,
				policy_node(gfp, pol, numa_node_id()),
				policy_nodemask(gfp, pol));

	return page;
}
```

### 4.3 __alloc_pages

```c
struct page *__alloc_pages(gfp_t gfp, unsigned int order, int preferred_nid,
							nodemask_t *nodemask)
{
	struct page *page;
	unsigned int alloc_flags = ALLOC_WMARK_LOW;//分配标志
	gfp_t alloc_gfp; /* The gfp_t that was actually used for allocation */
	struct alloc_context ac = { };

	/*
	 * There are several places where we assume that the order value is sane
	 * so bail out early if the request is out of bound.
	 */
	if (WARN_ON_ONCE_GFP(order >= MAX_ORDER, gfp))//如果order > MAX_ORDER(11)，则警告
		return NULL;

	gfp &= gfp_allowed_mask;
	/*
	 * Apply scoped allocation constraints. This is mainly about GFP_NOFS
	 * resp. GFP_NOIO which has to be inherited for all allocation requests
	 * from a particular context which has been marked by
	 * memalloc_no{fs,io}_{save,restore}. And PF_MEMALLOC_PIN which ensures
	 * movable zones are not used during allocation.
	 */
	gfp = current_gfp_context(gfp);//获取当前上下文的gfp
	alloc_gfp = gfp;
	if (!prepare_alloc_pages(gfp, order, preferred_nid, nodemask, &ac,
			&alloc_gfp, &alloc_flags))//如果没有准备好分配页面的条件则返回null
		return NULL;

	/*
	 * Forbid the first pass from falling back to types that fragment
	 * memory until all local zones are considered.
	 */
	alloc_flags |= alloc_flags_nofragment(ac.preferred_zoneref->zone, gfp);//设置分配的标志

	/* First allocation attempt */
    //调用get_page_from_freelist从空闲链表获得空闲页
	page = get_page_from_freelist(alloc_gfp, order, alloc_flags, &ac);
	if (likely(page))//成功获得，跳到out执行
		goto out;
	//分配失败，重置alloc_gfp
	alloc_gfp = gfp;
	ac.spread_dirty_pages = false;

	/*
	 * Restore the original nodemask if it was potentially replaced with
	 * &cpuset_current_mems_allowed to optimize the fast-path attempt.
	 */
	ac.nodemask = nodemask;
	//调用__alloc_pages_slowpath分配页面
	page = __alloc_pages_slowpath(alloc_gfp, order, &ac);

out:
    //判定页面是否满足分配的条件
	if (memcg_kmem_enabled() && (gfp & __GFP_ACCOUNT) && page &&
	    unlikely(__memcg_kmem_charge_page(page, gfp, order) != 0)) {
		__free_pages(page, order);//释放内存
		page = NULL;
	}

	trace_mm_page_alloc(page, order, alloc_gfp, ac.migratetype);//跟踪页面分配的信息
	kmsan_alloc_page(page, order, alloc_gfp);//将分配的页面地址映射到内核空间

	return page;//返回分配的页面地址
}
```

### 4.4 get_page_from_freelist

```c
get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,
						const struct alloc_context *ac)
{
	struct zoneref *z;
	struct zone *zone;
	struct pglist_data *last_pgdat = NULL;
	bool last_pgdat_dirty_ok = false;
	bool no_fallback;

retry:
	/*
	 * Scan zonelist, looking for a zone with enough free.
	 * See also __cpuset_node_allowed() comment in kernel/cgroup/cpuset.c.
	 */
    //扫描zonelist，区链表，寻找一个空闲的区
	no_fallback = alloc_flags & ALLOC_NOFRAGMENT;
	z = ac->preferred_zoneref;
	for_next_zone_zonelist_nodemask(zone, z, ac->highest_zoneidx,
					ac->nodemask) {
		struct page *page;
		unsigned long mark;
        
		//分配标志不允许在该区中分配页面
		if (cpusets_enabled() &&
			(alloc_flags & ALLOC_CPUSET) &&
			!__cpuset_zone_allowed(zone, gfp_mask))
				continue;
		/*
		 * When allocating a page cache page for writing, we
		 * want to get it from a node that is within its dirty
		 * limit, such that no single node holds more than its
		 * proportional share of globally allowed dirty pages.
		 * The dirty limits take into account the node's
		 * lowmem reserves and high watermark so that kswapd
		 * should be able to balance it without having to
		 * write pages from its LRU list.
		 *
		 * XXX: For now, allow allocations to potentially
		 * exceed the per-node dirty limit in the slowpath
		 * (spread_dirty_pages unset) before going into reclaim,
		 * which is important when on a NUMA setup the allowed
		 * nodes are together not big enough to reach the
		 * global limit.  The proper fix for these situations
		 * will require awareness of nodes in the
		 * dirty-throttling and the flusher threads.
		 */
        //如果考虑该节点上的脏页分布
		if (ac->spread_dirty_pages) {
      	//判断当前区域的zone_pgdat是否是last_pgdat 
			if (last_pgdat != zone->zone_pgdat) {
				last_pgdat = zone->zone_pgdat;//更新last_pgdat以及last_pgdat_dirty_ok
				last_pgdat_dirty_ok = node_dirty_ok(zone->zone_pgdat);
			}

			if (!last_pgdat_dirty_ok)
				continue;
		}

		if (no_fallback && nr_online_nodes > 1 &&
		    zone != ac->preferred_zoneref->zone) {
			int local_nid;

			/*
			 * If moving to a remote node, retry but allow
			 * fragmenting fallbacks. Locality is more important
			 * than fragmentation avoidance.
			 */
			local_nid = zone_to_nid(ac->preferred_zoneref->zone);
            //如果不是local节点，则执行retry
			if (zone_to_nid(zone) != local_nid) {
				alloc_flags &= ~ALLOC_NOFRAGMENT;
				goto retry;
			}
		}

		mark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK);
		if (!zone_watermark_fast(zone, order, mark,
				       ac->highest_zoneidx, alloc_flags,
				       gfp_mask)) {
			int ret;
			/* Checked here to keep the fast path fast */
			BUILD_BUG_ON(ALLOC_NO_WATERMARKS < NR_WMARK);
            //如果设置了ALLOC_NO_WATERMARKS，可以在当前区使用伙伴算法
			if (alloc_flags & ALLOC_NO_WATERMARKS)
				goto try_this_zone;
			if (!node_reclaim_enabled() ||
			    !zone_allows_reclaim(ac->preferred_zoneref->zone, zone))
				continue;

			ret = node_reclaim(zone->zone_pgdat, gfp_mask, order);//该节点上实行内存回收函数
			switch (ret) {
			case NODE_RECLAIM_NOSCAN:
				/* 没有扫描回收 */
				continue;
			case NODE_RECLAIM_FULL:
				/*扫描了但是没有回收 */
				continue;
			default:
				/* 如果空闲物理页数充足，则可以在当前区使用伙伴算法 */
				if (zone_watermark_ok(zone, order, mark,
					ac->highest_zoneidx, alloc_flags))
					goto try_this_zone;

				continue;
			}
		}

try_this_zone:
        //调用rmqueue来分配页块
		page = rmqueue(ac->preferred_zoneref->zone, zone, order,
				gfp_mask, alloc_flags, ac->migratetype);
        //如果分配页块成功
		if (page) {
            //准备分配新的页面
			prep_new_page(page, order, gfp_mask, alloc_flags);

            //如果设置 ALLOC_HARDER并且order不为0，调用reserve_highatomic_pageblock，检查是否还继续保存high-order atomic allocation
			if (unlikely(order && (alloc_flags & ALLOC_HARDER)))
				reserve_highatomic_pageblock(page, zone, order);

			return page;
		}
	}

	/*
	 * It's possible on a UMA machine to get through all zones that are
	 * fragmented. If avoiding fragmentation, reset and try again.
	 */
	if (no_fallback) {
        //如果设置ALLOC_NOFRAGMENT，则清此标志位，跳转到retry
		alloc_flags &= ~ALLOC_NOFRAGMENT;
		goto retry;
	}

	return NULL;
}
```

### 4.5 rmqueue

```c
static inline struct page *rmqueue(struct zone *preferred_zone,struct zone *zone, unsigned int order,gfp_t gfp_flags, unsigned int alloc_flags,int migratetype)
{
	struct page *page;

	/*
	 * We most definitely don't want callers attempting to
	 * allocate greater than order-1 page units with __GFP_NOFAIL.
	 */
    
    //如果调用者想申请大于2个页面的页块，则发出警告
	WARN_ON_ONCE((gfp_flags & __GFP_NOFAIL) && (order > 1));
	
    //检查是否允许从pcp链表中分配
	if (likely(pcp_allowed_order(order))) {
		/*
		 * MIGRATE_MOVABLE pcplist 不会有页表存在于CMA区，并且当CMA区不被允许，我们需要跳过它
		 */
        //如果CMA area不允许或者迁移类型不是 MIGRATE_MOVABLE，或者ALLOC_CMA 存在，则分配该页面
		if (!IS_ENABLED(CONFIG_CMA) || alloc_flags & ALLOC_CMA ||
				migratetype != MIGRATE_MOVABLE) {
            //调用rmqueue_pcplist来分配指定大小，类型的内存页面
			page = rmqueue_pcplist(preferred_zone, zone, order,
					migratetype, alloc_flags);
            
            //跳转到out执行
			if (likely(page))
				goto out;
		}
	}
//不允许从pcp链表中分配，则调用rmqueue_buddy来分配页面
	page = rmqueue_buddy(preferred_zone, zone, order, alloc_flags,
							migratetype);

out:
	/* Separate test+clear to avoid unnecessary atomics */
    //检查ZONE_BOOSTED_WATERMARK是否被设置了，如果被设置了
	if (unlikely(test_bit(ZONE_BOOSTED_WATERMARK, &zone->flags))) {
		clear_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);//ZONE_BOOSTED_WATERMARK设为0
        ////调用wakeup_kswapd唤醒内核交换守护进程,让其将内核中的某些页交换到外存，从而保证系统内部有足够多的空闲页块
		wakeup_kswapd(zone, 0, 0, zone_idx(zone));
	}
	
	VM_BUG_ON_PAGE(page && bad_range(zone, page), page);
	return page;
}
```

### 4.6 rmqueue_buddy

```c
static __always_inline struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,unsigned int order, unsigned int alloc_flags, int migratetype)
{
	struct page *page;
	unsigned long flags;
	do {
		page = NULL;
		spin_lock_irqsave(&zone->lock, flags);//打开当前区域的自旋锁
		/*
		 * order-0 request can reach here when the pcplist is skipped
		 * due to non-CMA allocation context. HIGHATOMIC area is
		 * reserved for high-order atomic allocation, so order-0
		 * request should skip it.
		 */
        //如果申请大于一个页面的页块且设置了ALLOC_HARDER
		if (order > 0 && alloc_flags & ALLOC_HARDER)
            //调用 __rmqueue_smallest来分配页块
			page = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);
        //未成功分配页块，则调用__rmqueue来分配
		if (!page) {
			page = __rmqueue(zone, order, migratetype, alloc_flags);
			if (!page) {
                //如果还未成功分配页块，则解开当前区的自旋锁，返回NULL
				spin_unlock_irqrestore(&zone->lock, flags);
				return NULL;
			}
		}
        //成功分配页块，解开当前区的自旋锁
		__mod_zone_freepage_state(zone, -(1 << order),
					  get_pcppage_migratetype(page));
		spin_unlock_irqrestore(&zone->lock, flags);
	} while (check_new_pages(page, order));

	__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);
	zone_statistics(preferred_zone, zone, 1);

	return page;
}

```

### 4.7 __rmqueue_smallest

```c
/*
 * Go through the free lists for the given migratetype and remove
 * the smallest available page from the freelists
 */
static __always_inline struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,int migratetype)
{
	unsigned int current_order;
	struct free_area *area;
	struct page *page;

	/* 在preferred list中找到一个合适大小的页块 */
	for (current_order = order; current_order < MAX_ORDER; ++current_order) {
        //area指向了当前区的free_area[current_order]
		area = &(zone->free_area[current_order]);
        //从空闲链表获取空闲页
		page = get_page_from_free_area(area, migratetype);
		if (!page)
			continue;
        //成功获取到空闲页
       //把这个页从空闲页中删除
		del_page_from_free_list(page, zone, current_order);
        //进行扩展页的大小
		expand(zone, page, order, current_order, migratetype);
        //设置页的迁移类型
		set_pcppage_migratetype(page, migratetype);
		trace_mm_page_alloc_zone_locked(page, order, migratetype,
				pcp_allowed_order(order) &&
				migratetype < MIGRATE_PCPTYPES);
		return page;
	}

	return NULL;
}
```

### 4.8 expand

```c
static inline void expand(struct zone *zone, struct page *page,int low, int high, int migratetype)//指向内存区，页块，当前页块的页面个数，要扩展的大小，迁移类型
{
    //size=2^high，用前面的例子512，分配128,size=512
	unsigned long size = 1 << high;
    
//当high>low,执行high--,size*2 size=size/(2^(high-low))
	while (high > low) {
		high--;
		size >>= 1;//size=512/(4)=128
		VM_BUG_ON_PAGE(bad_range(zone, &page[size]), &page[size]);

		/*
		 * Mark as guard pages (or page), that will allow to
		 * merge back to allocator when buddy will be freed.
		 * Corresponding page table entries will not be touched,
		 * pages will stay not present in virtual address space
		 */
		if (set_page_guard(zone, &page[size], high, migratetype))
			continue;
		//把页面128-512的块添加到空闲链表中
		add_to_free_list(&page[size], zone, high, migratetype);
		set_buddy_order(&page[size], high);
	}
}

```

## 五、内存泄漏实验

### 5.1 内存泄漏是什么

程序向系统申请内存，使用完不需要之后,不释放内存还给系统回收，造成申请的内存被浪费

### 5.2 实验部分

#### 5.2.1 编写代码mem_check.c

```c
#include <stdio.h>
#include <stdlib.h>
 
int main(){
    char *p1 = NULL;
    char *p2 = NULL;
    
    for(int i = 0; i < 5; i++) 
    {       
        p1 = malloc(16);
    }
 
    for(int i = 0; i < 5; i++)
    {
        p2 = malloc(32);
        free(p2);
    }
    getchar();
    return 0;
}
```

在mem_check.c里面，p1申请的内存没有被释放，存在内存泄漏。并且，只有p2申请的内存被释放了。

编译文件：`gcc mem_check.c -o mem_check`，执行`sudo bpftrace -e 'uprobe:/home/gyx/Desktop/mem_leak/mem_check:malloc {printf("malloc call\n")}'`报错信息如下：

![Buddy_8](.\img\Buddy_8.png)

报错内容表示可执行文件mem_check中找不到符号：于是使用ldd来查看malloc的链接库

![Buddy_9](.\img\Buddy_9.png)

mem_check可执行文件使用的C库为：/lib/x86_64-linux-gnu/libc.so.6，我们将可以执行文件替换为/lib/x86_64-linux-gnu/libc.so.6。再次执行，会出现大量内容，显然是其他进程调用了malloc引起的，而我们的mem_ckeck还没有运行，显然还没有探测我们的可执行程序，执行`sudo bpftrace -e 'uprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc {printf("malloc call\n")}'`

![Buddy_10](.\img\Buddy_10.png)

我们需要进行过滤，增加filter只保留我们关心的应用程序的调用探测。bpftrace提供了系统变量comm表示可执行文件名 (进程名)，只需要在上述指令中增加 filter，只处理comm=="mem_check"的malloc调用事件。左边终端执行探测，右边终端执行可执行文件。每调用一次malloc函数，就能探测到一次，命令如下：

`sudo bpftrace -e 'uprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc /comm="mem_check" /{printf("malloc call\n")}'`

![Buddy_11](.\img\Buddy_11.png)

### 5.3 使用bpftrace脚本进一步探测

#### 5.3.1探测mem_check中malloc的内存空间大小

```c
BEGIN {
    printf("start probe\n");
}
 
uprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc /comm == "mem_check"/{
  printf("malloc size: %d\n", arg0);
}
 
END {
    printf("end probe\n");
}
```

![Buddy_12](.\img\Buddy_12.png)

#### 5.3.2 探测mem_check中malloc的返回值

```c
BEGIN {
    printf("start probe\n");
}
 
uprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc /comm == "mem_check"/{
    printf("malloc size: %d\n", arg0);
}
 
uretprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc /comm == "mem_check"/{
  printf("addr = %p\n", retval);
}
 
END {
    printf("end probe\n");
}
```

![Buddy_13](.\img\Buddy_13.png)

#### 5.3.3 探测mem_check中free掉的地址

```c
BEGIN {
    printf("start probe\n");
}
 
uprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc /comm == "mem_check"/{
    printf("malloc size: %d\n", arg0);
}
 
uretprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc /comm == "mem_check"/{
  printf("addr = %p\n", retval);
}
 
uprobe:/lib/x86_64-linux-gnu/libc.so.6:free /comm == "mem_check"/{
  printf("free addr = %p\n", arg0);
}
 
END {
    printf("end probe\n");
}
```

![Buddy_14](.\img\Buddy_14.png)

#### 5.3.4 探测内存泄露地址大小

```c
BEGIN {
    printf("start probe\n");
}
 
uprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc /comm == "mem_check"/{
    printf("malloc size: %d\n", arg0);
    @size = arg0;
}
 
uretprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc /comm == "mem_check"/{
    printf("addr = %p\n", retval);
    @mem[retval] = @size;
 
}
 
uprobe:/lib/x86_64-linux-gnu/libc.so.6:free /comm == "mem_check"/{
    printf("free addr = %p\n", arg0);
    delete(@mem[arg0]);
}
 
END {
    printf("end probe\n");
}
```

![Buddy_15](.\img\Buddy_15.png)

## 五、心得体会

通过对伙伴算法的源码分析，我发现对于内核的学习，是永无止境且必须要实践的，在理论上看起来伙伴算法的实现很简单，但从代码实现来说真的不简单。我觉得分析源码的过程对于自己代码阅读能力的提升很有帮助，我发现自己还是有很多地方分析的不到位，只是分析了有关伙伴算法的主干，没有特别详细的看。关于源码的学习还必须进一步深入
